{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe7f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity df columns: Index(['id', 'human_readable_id', 'title', 'type', 'description',\n",
      "       'text_unit_ids', 'frequency', 'degree', 'x', 'y'],\n",
      "      dtype='object')\n",
      "Entity count: 1824\n",
      "Relationship count: 2641\n",
      "Text unit records: 243\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>document_ids</th>\n",
       "      <th>entity_ids</th>\n",
       "      <th>relationship_ids</th>\n",
       "      <th>covariate_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>422c343a682e7f78dae36ac94f2bf135ec09bd2784744d...</td>\n",
       "      <td>1</td>\n",
       "      <td>Tell us about your PDF experience.\\nMicrosoft ...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[d5835eb381e6b16dd4dd9c9b74c2bb54d06463d791e93...</td>\n",
       "      <td>[3e730b25-df4d-4390-a891-3cb559ab387a, 09903b4...</td>\n",
       "      <td>[3877ccd6-2ae9-46bc-82de-3acf47ff371e, abb9d1b...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7f05149428714a222f2c4fa84b8dc05f8991a7a9266cab...</td>\n",
       "      <td>2</td>\n",
       "      <td>ises and in the cloud. For more information, s...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[d5835eb381e6b16dd4dd9c9b74c2bb54d06463d791e93...</td>\n",
       "      <td>[3e730b25-df4d-4390-a891-3cb559ab387a, 09903b4...</td>\n",
       "      <td>[3877ccd6-2ae9-46bc-82de-3acf47ff371e, 6b4b719...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>406c2d7deeffa71c9bcd8922a425f6548d58eabd180b0d...</td>\n",
       "      <td>3</td>\n",
       "      <td>IoT Hub, Azure SQL DB Change Data Capture (CD...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[d5835eb381e6b16dd4dd9c9b74c2bb54d06463d791e93...</td>\n",
       "      <td>[3e730b25-df4d-4390-a891-3cb559ab387a, 09903b4...</td>\n",
       "      <td>[3877ccd6-2ae9-46bc-82de-3acf47ff371e, ba7b4a6...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03b9935515464b3ba9fa660b98d2b7bd12a24ebe4f6494...</td>\n",
       "      <td>4</td>\n",
       "      <td>For detailed instructions, see\\nMoving your d...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[d5835eb381e6b16dd4dd9c9b74c2bb54d06463d791e93...</td>\n",
       "      <td>[3e730b25-df4d-4390-a891-3cb559ab387a, 5bffd67...</td>\n",
       "      <td>[ba7b4a6c-1a26-45b7-a0fa-e3dfd981aa88, 6b4b719...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0c2447dd1987bf104efc25070271e6f085d4b9c2945d60...</td>\n",
       "      <td>5</td>\n",
       "      <td>, see Canceling, expiring, and closing.\\nCance...</td>\n",
       "      <td>1200</td>\n",
       "      <td>[d5835eb381e6b16dd4dd9c9b74c2bb54d06463d791e93...</td>\n",
       "      <td>[5bffd675-01c6-4f6d-a247-99333ee64f05, c44a355...</td>\n",
       "      <td>[0ad8a4e0-cedf-41fa-bc40-0a41babad623, f08cbdf...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  human_readable_id  \\\n",
       "0  422c343a682e7f78dae36ac94f2bf135ec09bd2784744d...                  1   \n",
       "1  7f05149428714a222f2c4fa84b8dc05f8991a7a9266cab...                  2   \n",
       "2  406c2d7deeffa71c9bcd8922a425f6548d58eabd180b0d...                  3   \n",
       "3  03b9935515464b3ba9fa660b98d2b7bd12a24ebe4f6494...                  4   \n",
       "4  0c2447dd1987bf104efc25070271e6f085d4b9c2945d60...                  5   \n",
       "\n",
       "                                                text  n_tokens  \\\n",
       "0  Tell us about your PDF experience.\\nMicrosoft ...      1200   \n",
       "1  ises and in the cloud. For more information, s...      1200   \n",
       "2   IoT Hub, Azure SQL DB Change Data Capture (CD...      1200   \n",
       "3   For detailed instructions, see\\nMoving your d...      1200   \n",
       "4  , see Canceling, expiring, and closing.\\nCance...      1200   \n",
       "\n",
       "                                        document_ids  \\\n",
       "0  [d5835eb381e6b16dd4dd9c9b74c2bb54d06463d791e93...   \n",
       "1  [d5835eb381e6b16dd4dd9c9b74c2bb54d06463d791e93...   \n",
       "2  [d5835eb381e6b16dd4dd9c9b74c2bb54d06463d791e93...   \n",
       "3  [d5835eb381e6b16dd4dd9c9b74c2bb54d06463d791e93...   \n",
       "4  [d5835eb381e6b16dd4dd9c9b74c2bb54d06463d791e93...   \n",
       "\n",
       "                                          entity_ids  \\\n",
       "0  [3e730b25-df4d-4390-a891-3cb559ab387a, 09903b4...   \n",
       "1  [3e730b25-df4d-4390-a891-3cb559ab387a, 09903b4...   \n",
       "2  [3e730b25-df4d-4390-a891-3cb559ab387a, 09903b4...   \n",
       "3  [3e730b25-df4d-4390-a891-3cb559ab387a, 5bffd67...   \n",
       "4  [5bffd675-01c6-4f6d-a247-99333ee64f05, c44a355...   \n",
       "\n",
       "                                    relationship_ids covariate_ids  \n",
       "0  [3877ccd6-2ae9-46bc-82de-3acf47ff371e, abb9d1b...            []  \n",
       "1  [3877ccd6-2ae9-46bc-82de-3acf47ff371e, 6b4b719...            []  \n",
       "2  [3877ccd6-2ae9-46bc-82de-3acf47ff371e, ba7b4a6...            []  \n",
       "3  [ba7b4a6c-1a26-45b7-a0fa-e3dfd981aa88, 6b4b719...            []  \n",
       "4  [0ad8a4e0-cedf-41fa-bc40-0a41babad623, f08cbdf...            []  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "from graphrag.config.enums import ModelType\n",
    "from graphrag.config.models.drift_search_config import DRIFTSearchConfig\n",
    "from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "from graphrag.language_model.manager import ModelManager\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_report_embeddings,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "from graphrag.query.structured_search.drift_search.drift_context import (\n",
    "    DRIFTSearchContextBuilder,\n",
    ")\n",
    "from graphrag.query.structured_search.drift_search.search import DRIFTSearch\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "INPUT_DIR = \"./output\"\n",
    "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
    "\n",
    "COMMUNITY_REPORT_TABLE = \"community_reports\"\n",
    "COMMUNITY_TABLE = \"communities\"\n",
    "ENTITY_TABLE = \"entities\"\n",
    "RELATIONSHIP_TABLE = \"relationships\"\n",
    "#COVARIATE_TABLE = \"covariates\"\n",
    "TEXT_UNIT_TABLE = \"text_units\"\n",
    "COMMUNITY_LEVEL = 2\n",
    "\n",
    "\n",
    "# read nodes table to get community and degree data\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "community_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_TABLE}.parquet\")\n",
    "\n",
    "print(f\"Entity df columns: {entity_df.columns}\")\n",
    "\n",
    "entities = read_indexer_entities(entity_df, community_df, COMMUNITY_LEVEL)\n",
    "\n",
    "# load description embeddings to an in-memory lancedb vectorstore\n",
    "# to connect to a remote db, specify url and port values.\n",
    "description_embedding_store = LanceDBVectorStore(\n",
    "    collection_name=\"default-entity-description\",\n",
    ")\n",
    "description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "full_content_embedding_store = LanceDBVectorStore(\n",
    "    collection_name=\"default-community-full_content\",\n",
    ")\n",
    "full_content_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "print(f\"Entity count: {len(entity_df)}\")\n",
    "entity_df.head()\n",
    "\n",
    "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet\")\n",
    "relationships = read_indexer_relationships(relationship_df)\n",
    "\n",
    "print(f\"Relationship count: {len(relationship_df)}\")\n",
    "relationship_df.head()\n",
    "\n",
    "text_unit_df = pd.read_parquet(f\"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet\")\n",
    "text_units = read_indexer_text_units(text_unit_df)\n",
    "\n",
    "print(f\"Text unit records: {len(text_unit_df)}\")\n",
    "text_unit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9352ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedback why should we pass the type to both config and the chat_model etc?\n",
    "\n",
    "from graphrag.config.enums import ModelType, AuthType\n",
    "from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "from graphrag.language_model.manager import ModelManager\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GRAPHRAG_API_KEY\")\n",
    "llm_model = os.getenv(\"GRAPHRAG_LLM_MODEL\")\n",
    "embedding_model = os.getenv(\"GRAPHRAG_EMBEDDING_MODEL\")\n",
    "\n",
    "chat_config = LanguageModelConfig(\n",
    "    api_key=api_key,\n",
    "    auth_type=AuthType.APIKey, \n",
    "    type=ModelType.AzureOpenAIChat,\n",
    "    model=llm_model,\n",
    "    deployment_name=llm_model,\n",
    "    max_retries=20,\n",
    "    api_base= os.getenv(\"GRAPHRAG_API_BASE\"),\n",
    "    api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "chat_model = ModelManager().get_or_create_chat_model(\n",
    "    name=\"local_search\",\n",
    "    model_type=ModelType.AzureOpenAIChat,\n",
    "    config=chat_config,\n",
    ")\n",
    "\n",
    "token_encoder = tiktoken.encoding_for_model(llm_model)\n",
    "\n",
    "embedding_config = LanguageModelConfig(\n",
    "    api_key=api_key,\n",
    "    auth_type=AuthType.APIKey,\n",
    "    type=ModelType.AzureOpenAIEmbedding,  # <-- Switch to AzureOpenAIEmbedding\n",
    "    model=embedding_model,                # <-- This should be your Azure deployment name for embeddings\n",
    "    deployment_name=embedding_model,      # <-- Same as above\n",
    "    api_base=os.getenv(\"GRAPHRAG_API_BASE\"),\n",
    "    api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "text_embedder = ModelManager().get_or_create_embedding_model(\n",
    "    name=\"local_search_embedding\",\n",
    "    model_type=ModelType.AzureOpenAIEmbedding,\n",
    "    config=embedding_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310f62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_community_reports(\n",
    "    input_dir: str,\n",
    "    community_report_table: str = COMMUNITY_REPORT_TABLE,\n",
    "):\n",
    "    \"\"\"Embeds the full content of the community reports and saves the DataFrame with embeddings to the output path.\"\"\"\n",
    "    input_path = Path(input_dir) / f\"{community_report_table}.parquet\"\n",
    "    return pd.read_parquet(input_path)\n",
    "\n",
    "\n",
    "report_df = read_community_reports(INPUT_DIR)\n",
    "reports = read_indexer_reports(\n",
    "    report_df,\n",
    "    community_df,\n",
    "    COMMUNITY_LEVEL,\n",
    "    content_embedding_col=\"full_content_embeddings\",\n",
    ")\n",
    "read_indexer_report_embeddings(reports, full_content_embedding_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39835509",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_params = DRIFTSearchConfig(\n",
    "    temperature=0,\n",
    "    max_tokens=12_000,\n",
    "    primer_folds=1,\n",
    "    drift_k_followups=3,\n",
    "    n_depth=3,\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "context_builder = DRIFTSearchContextBuilder(\n",
    "    model=chat_model,\n",
    "    text_embedder=text_embedder,\n",
    "    entities=entities,\n",
    "    relationships=relationships,\n",
    "    reports=reports,\n",
    "    entity_text_embeddings=description_embedding_store,\n",
    "    text_units=text_units,\n",
    "    token_encoder=token_encoder,\n",
    "    config=drift_params,\n",
    ")\n",
    "\n",
    "search = DRIFTSearch(\n",
    "    model=chat_model, context_builder=context_builder, token_encoder=token_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc23774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AsyncCompletions.create() got an unexpected keyword argument 'model_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m search.search(\u001b[33m\"\u001b[39m\u001b[33mhow do you do shortcuts?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ai-agents/lib/python3.12/site-packages/graphrag/query/structured_search/drift_search/search.py:279\u001b[39m, in \u001b[36mDRIFTSearch.search\u001b[39m\u001b[34m(self, query, conversation_history, reduce, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m     callback.on_reduce_response_start(response_state)\n\u001b[32m    272\u001b[39m model_params = get_openai_model_parameters_from_dict({\n\u001b[32m    273\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.model.config.model,\n\u001b[32m    274\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.context_builder.config.reduce_max_tokens,\n\u001b[32m    275\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.context_builder.config.reduce_temperature,\n\u001b[32m    276\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.context_builder.config.reduce_max_completion_tokens,\n\u001b[32m    277\u001b[39m })\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m reduced_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._reduce_response(\n\u001b[32m    280\u001b[39m     responses=response_state,\n\u001b[32m    281\u001b[39m     query=query,\n\u001b[32m    282\u001b[39m     llm_calls=llm_calls,\n\u001b[32m    283\u001b[39m     prompt_tokens=prompt_tokens,\n\u001b[32m    284\u001b[39m     output_tokens=output_tokens,\n\u001b[32m    285\u001b[39m     model_params=model_params,\n\u001b[32m    286\u001b[39m )\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m    289\u001b[39m     callback.on_reduce_response_end(reduced_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ai-agents/lib/python3.12/site-packages/graphrag/query/structured_search/drift_search/search.py:386\u001b[39m, in \u001b[36mDRIFTSearch._reduce_response\u001b[39m\u001b[34m(self, responses, query, llm_calls, prompt_tokens, output_tokens, **llm_kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m search_prompt = \u001b[38;5;28mself\u001b[39m.context_builder.reduce_system_prompt.format(\n\u001b[32m    379\u001b[39m     context_data=reduce_responses,\n\u001b[32m    380\u001b[39m     response_type=\u001b[38;5;28mself\u001b[39m.context_builder.response_type,\n\u001b[32m    381\u001b[39m )\n\u001b[32m    382\u001b[39m search_messages = [\n\u001b[32m    383\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: search_prompt},\n\u001b[32m    384\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m model_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.achat(\n\u001b[32m    387\u001b[39m     prompt=query,\n\u001b[32m    388\u001b[39m     history=search_messages,\n\u001b[32m    389\u001b[39m     model_parameters=llm_kwargs,\n\u001b[32m    390\u001b[39m )\n\u001b[32m    392\u001b[39m reduced_response = model_response.output.content\n\u001b[32m    394\u001b[39m llm_calls[\u001b[33m\"\u001b[39m\u001b[33mreduce\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ai-agents/lib/python3.12/site-packages/graphrag/language_model/providers/fnllm/models.py:283\u001b[39m, in \u001b[36mAzureOpenAIChatFNLLM.achat\u001b[39m\u001b[34m(self, prompt, history, **kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model(prompt, **kwargs)\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model(prompt, history=history, **kwargs)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelResponse(\n\u001b[32m    285\u001b[39m     output=BaseModelOutput(content=response.output.content),\n\u001b[32m    286\u001b[39m     parsed_response=response.parsed_json,\n\u001b[32m   (...)\u001b[39m\u001b[32m    290\u001b[39m     metrics=response.metrics,\n\u001b[32m    291\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ai-agents/lib/python3.12/site-packages/fnllm/openai/llm/openai_chat_llm.py:94\u001b[39m, in \u001b[36mOpenAIChatLLMImpl.__call__\u001b[39m\u001b[34m(self, prompt, stream, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._streaming_chat_llm(prompt, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._text_chat_llm(prompt, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ai-agents/lib/python3.12/site-packages/fnllm/openai/services/openai_tools_parsing.py:130\u001b[39m, in \u001b[36mOpenAIParseToolsLLM.__call__\u001b[39m\u001b[34m(self, prompt, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m tools = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tools:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._delegate(prompt, **kwargs)\n\u001b[32m    132\u001b[39m completion_parameters = \u001b[38;5;28mself\u001b[39m._add_tools_to_parameters(kwargs, tools)\n\u001b[32m    134\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._delegate(prompt, **completion_parameters)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ai-agents/lib/python3.12/site-packages/fnllm/base/base_llm.py:144\u001b[39m, in \u001b[36mBaseLLM.__call__\u001b[39m\u001b[34m(self, prompt, **kwargs)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    143\u001b[39m     prompt, kwargs = \u001b[38;5;28mself\u001b[39m._rewrite_input(prompt, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decorated_target(prompt, **kwargs)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    146\u001b[39m     stack_trace = traceback.format_exc()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ai-agents/lib/python3.12/site-packages/fnllm/base/services/json.py:78\u001b[39m, in \u001b[36mJsonReceiver.decorate.<locals>.invoke\u001b[39m\u001b[34m(prompt, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mjson_model\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m this.invoke_json(delegate, prompt, kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m delegate(prompt, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ai-agents/lib/python3.12/site-packages/fnllm/base/services/rate_limiter.py:75\u001b[39m, in \u001b[36mRateLimiter.decorate.<locals>.invoke\u001b[39m\u001b[34m(prompt, **args)\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._limiter.use(manifest):\n\u001b[32m     74\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events.on_limit_acquired(manifest)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m delegate(prompt, **args)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events.on_limit_released(manifest)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ai-agents/lib/python3.12/site-packages/fnllm/base/base_llm.py:126\u001b[39m, in \u001b[36mBaseLLM._decorator_target\u001b[39m\u001b[34m(self, prompt, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Target for the decorator chain.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m    123\u001b[39m \u001b[33;03mLeave signature alone as prompt,  kwargs.\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events.on_execute_llm()\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m output = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._execute_llm(prompt, kwargs)\n\u001b[32m    127\u001b[39m result = LLMOutput(output=output)\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inject_usage(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ai-agents/lib/python3.12/site-packages/fnllm/openai/llm/openai_text_chat_llm.py:157\u001b[39m, in \u001b[36mOpenAITextChatLLMImpl._execute_llm\u001b[39m\u001b[34m(self, prompt, kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m local_model_parameters = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mmodel_parameters\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    155\u001b[39m parameters = \u001b[38;5;28mself\u001b[39m._build_completion_parameters(local_model_parameters)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m completion = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIterator\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionMessageParam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m result = completion.choices[\u001b[32m0\u001b[39m].message\n\u001b[32m    163\u001b[39m usage: LLMUsageMetrics | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/ai-agents/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: AsyncCompletions.create() got an unexpected keyword argument 'model_params'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result = await search.search(\"how do you do shortcuts?\")\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef299fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.context_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

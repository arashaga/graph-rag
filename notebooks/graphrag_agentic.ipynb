{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738de017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ai-agents/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from graphrag.config.enums import ModelType, AuthType\n",
    "from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "from graphrag.language_model.manager import ModelManager\n",
    "from crewai import Agent, Task, Crew, LLM\n",
    "from crewai.tools import BaseTool\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    "    read_indexer_communities,\n",
    ")\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n",
    "from graphrag.query.structured_search.local_search.mixed_context import LocalSearchMixedContext\n",
    "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
    "from graphrag.query.structured_search.global_search.community_context import GlobalCommunityContext\n",
    "from graphrag.query.structured_search.global_search.search import GlobalSearch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GRAPHRAG_API_KEY\")\n",
    "llm_model = os.getenv(\"GRAPHRAG_LLM_MODEL\")\n",
    "embedding_model = os.getenv(\"GRAPHRAG_EMBEDDING_MODEL\")\n",
    "api_base = os.getenv(\"GRAPHRAG_API_BASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc276e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRagBaseConfig:\n",
    "    def __init__(self, llm_model, embedding_model, api_key, api_base):\n",
    "        self.llm_model = llm_model\n",
    "        self.embedding_model = embedding_model\n",
    "        self.api_key = api_key\n",
    "        self.api_base = api_base\n",
    "\n",
    "    def get_chat_model(self, name=\"default\"):\n",
    "        chat_config = LanguageModelConfig(\n",
    "            api_key=self.api_key,\n",
    "            auth_type=AuthType.APIKey,\n",
    "            type=ModelType.AzureOpenAIChat,\n",
    "            model=self.llm_model,\n",
    "            deployment_name=self.llm_model,\n",
    "            max_retries=20,\n",
    "            api_base=self.api_base,\n",
    "            api_version=\"2024-02-15-preview\"\n",
    "        )\n",
    "        return ModelManager().get_or_create_chat_model(\n",
    "            name=name,\n",
    "            model_type=ModelType.AzureOpenAIChat,\n",
    "            config=chat_config\n",
    "        )\n",
    "\n",
    "    def get_embedding_model(self, name=\"embedding\"):\n",
    "        embedding_config = LanguageModelConfig(\n",
    "            api_key=self.api_key,\n",
    "            auth_type=AuthType.APIKey,\n",
    "            type=ModelType.AzureOpenAIEmbedding,\n",
    "            model=self.embedding_model,\n",
    "            deployment_name=self.embedding_model,\n",
    "            api_base=self.api_base,\n",
    "            api_version=\"2024-02-15-preview\"\n",
    "        )\n",
    "        return ModelManager().get_or_create_embedding_model(\n",
    "            name=name,\n",
    "            model_type=ModelType.AzureOpenAIEmbedding,\n",
    "            config=embedding_config\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc01344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GraphRagBaseConfig(llm_model, embedding_model, api_key, api_base)\n",
    "#chat_model = config.get_chat_model()\n",
    "#embedding_model = config.get_embedding_model()\n",
    "token_encoder = tiktoken.encoding_for_model(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb310fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LocalSearchTool(BaseTool):\n",
    "    name: str = \"local_search\"\n",
    "    description: str = \"Tool to perform local search operations using GraphRAG's index.\"\n",
    "\n",
    "    # Declare all extra fields as Pydantic fields here\n",
    "    input_dir: str = \"./output/\"\n",
    "    llm_model: str = None\n",
    "    embedding_model: str = None\n",
    "    api_key: str = None\n",
    "    api_base: str = None   \n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dir=\"./output/\",\n",
    "        llm_model=None,\n",
    "        embedding_model=None,\n",
    "        api_key=None,\n",
    "        api_base=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dir = input_dir\n",
    "\n",
    "        # Setup paths and configs\n",
    "        LANCEDB_URI = f\"{self.input_dir}/lancedb\"\n",
    "        COMMUNITY_REPORT_TABLE = \"community_reports\"\n",
    "        ENTITY_TABLE = \"entities\"\n",
    "        COMMUNITY_TABLE = \"communities\"\n",
    "        RELATIONSHIP_TABLE = \"relationships\"\n",
    "        TEXT_UNIT_TABLE = \"text_units\"\n",
    "        COMMUNITY_LEVEL = 2\n",
    "\n",
    "        # Load DataFrames\n",
    "        entity_df = pd.read_parquet(f\"{self.input_dir}/{ENTITY_TABLE}.parquet\")\n",
    "        community_df = pd.read_parquet(f\"{self.input_dir}/{COMMUNITY_TABLE}.parquet\")\n",
    "        relationship_df = pd.read_parquet(f\"{self.input_dir}/{RELATIONSHIP_TABLE}.parquet\")\n",
    "        report_df = pd.read_parquet(f\"{self.input_dir}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "        text_unit_df = pd.read_parquet(f\"{self.input_dir}/{TEXT_UNIT_TABLE}.parquet\")\n",
    "\n",
    "        entities = read_indexer_entities(entity_df, community_df, COMMUNITY_LEVEL)\n",
    "        relationships = read_indexer_relationships(relationship_df)\n",
    "        reports = read_indexer_reports(report_df, community_df, COMMUNITY_LEVEL)\n",
    "        text_units = read_indexer_text_units(text_unit_df)\n",
    "\n",
    "        # Load LanceDB Vector Store\n",
    "        description_embedding_store = LanceDBVectorStore(\n",
    "            collection_name=\"default-entity-description\",\n",
    "        )\n",
    "        description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "        # Model and embedder\n",
    "        import tiktoken\n",
    "        from graphrag.config.enums import ModelType, AuthType\n",
    "        from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "        from graphrag.language_model.manager import ModelManager\n",
    "\n",
    "        token_encoder = tiktoken.encoding_for_model(llm_model)\n",
    "\n",
    "        # Reuse global configs if possible\n",
    "        chat_config = LanguageModelConfig(\n",
    "            api_key=api_key,\n",
    "            auth_type=AuthType.APIKey,\n",
    "            type=ModelType.AzureOpenAIChat,\n",
    "            model=llm_model,\n",
    "            deployment_name=llm_model,\n",
    "            max_retries=20,\n",
    "            api_base=api_base,\n",
    "            api_version=\"2024-02-15-preview\"\n",
    "        )\n",
    "        chat_model = ModelManager().get_or_create_chat_model(\n",
    "            name=\"local_search\",\n",
    "            model_type=ModelType.AzureOpenAIChat,\n",
    "            config=chat_config,\n",
    "        )\n",
    "\n",
    "        embedding_config = LanguageModelConfig(\n",
    "            api_key=api_key,\n",
    "            auth_type=AuthType.APIKey,\n",
    "            type=ModelType.AzureOpenAIEmbedding,\n",
    "            model=embedding_model,\n",
    "            deployment_name=embedding_model,\n",
    "            api_base=api_base,\n",
    "            api_version=\"2024-02-15-preview\"\n",
    "        )\n",
    "        text_embedder = ModelManager().get_or_create_embedding_model(\n",
    "            name=\"local_search_embedding\",\n",
    "            model_type=ModelType.AzureOpenAIEmbedding,\n",
    "            config=embedding_config,\n",
    "        )\n",
    "\n",
    "        # Context builder\n",
    "        _context_builder = LocalSearchMixedContext(\n",
    "            community_reports=reports,\n",
    "            text_units=text_units,\n",
    "            entities=entities,\n",
    "            relationships=relationships,\n",
    "            entity_text_embeddings=description_embedding_store,\n",
    "            embedding_vectorstore_key=EntityVectorStoreKey.ID,\n",
    "            text_embedder=text_embedder,\n",
    "            token_encoder=token_encoder,\n",
    "        )\n",
    "\n",
    "        local_context_params = {\n",
    "            \"text_unit_prop\": 0.5,\n",
    "            \"community_prop\": 0.1,\n",
    "            \"conversation_history_max_turns\": 5,\n",
    "            \"conversation_history_user_turns_only\": True,\n",
    "            \"top_k_mapped_entities\": 10,\n",
    "            \"top_k_relationships\": 10,\n",
    "            \"include_entity_rank\": True,\n",
    "            \"include_relationship_weight\": True,\n",
    "            \"include_community_rank\": False,\n",
    "            \"return_candidate_context\": False,\n",
    "            \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,\n",
    "            \"max_tokens\": 12_000,\n",
    "        }\n",
    "        model_params = {\n",
    "            \"max_tokens\": 2_000,\n",
    "            \"temperature\": 0.0,\n",
    "        }\n",
    "\n",
    "        self._search_engine = LocalSearch(\n",
    "            model=chat_model,\n",
    "            context_builder=_context_builder,\n",
    "            token_encoder=token_encoder,\n",
    "            model_params=model_params,\n",
    "            context_builder_params=local_context_params,\n",
    "            response_type=\"multiple paragraphs\"\n",
    "        )\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        import asyncio\n",
    "        try:\n",
    "            loop = asyncio.get_running_loop()\n",
    "        except RuntimeError:\n",
    "            loop = None\n",
    "\n",
    "        if loop and loop.is_running():\n",
    "            # If in Jupyter, nest coroutine with ensure_future and run until complete\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            future = asyncio.ensure_future(self._search_engine.search(query))\n",
    "            result = loop.run_until_complete(future)\n",
    "        else:\n",
    "            result = asyncio.run(self._search_engine.search(query))\n",
    "        return result.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f5fdc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GlobalSearchTool(BaseTool):\n",
    "    name: str = \"global_search\"\n",
    "    description: str = \"Tool to perform global search operations using GraphRAG's index.\"\n",
    "    input_dir: str = \"./output/\"\n",
    "    llm_model: str = None\n",
    "    api_key: str = None\n",
    "    api_base: str = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dir=\"./output\",\n",
    "        llm_model=None,\n",
    "        api_key=None,\n",
    "        api_base=None,\n",
    "        callbacks=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dir = input_dir\n",
    "\n",
    "        COMMUNITY_REPORT_TABLE = \"community_reports\"\n",
    "        ENTITY_TABLE = \"entities\"\n",
    "        COMMUNITY_TABLE = \"communities\"\n",
    "        COMMUNITY_LEVEL = 2\n",
    "\n",
    "        # Load DataFrames\n",
    "        community_df = pd.read_parquet(f\"{self.input_dir}/{COMMUNITY_TABLE}.parquet\")\n",
    "        entity_df = pd.read_parquet(f\"{self.input_dir}/{ENTITY_TABLE}.parquet\")\n",
    "        report_df = pd.read_parquet(f\"{self.input_dir}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "\n",
    "        communities = read_indexer_communities(community_df, report_df)\n",
    "        reports = read_indexer_reports(report_df, community_df, COMMUNITY_LEVEL)\n",
    "        entities = read_indexer_entities(entity_df, community_df, COMMUNITY_LEVEL)\n",
    "\n",
    "        token_encoder = tiktoken.encoding_for_model(llm_model)\n",
    "\n",
    "        # LLM Model Config\n",
    "        chat_config = LanguageModelConfig(\n",
    "            api_key=api_key,\n",
    "            auth_type=AuthType.APIKey,\n",
    "            type=ModelType.AzureOpenAIChat,\n",
    "            model=llm_model,\n",
    "            deployment_name=llm_model,\n",
    "            max_retries=20,\n",
    "            api_base=api_base,\n",
    "            api_version=\"2024-02-15-preview\"\n",
    "        )\n",
    "        chat_model = ModelManager().get_or_create_chat_model(\n",
    "            name=\"global_search\",\n",
    "            model_type=ModelType.AzureOpenAIChat,\n",
    "            config=chat_config,\n",
    "        )\n",
    "\n",
    "        # Context builder\n",
    "        _context_builder = GlobalCommunityContext(\n",
    "            community_reports=reports,\n",
    "            communities=communities,\n",
    "            entities=entities,\n",
    "            token_encoder=token_encoder,\n",
    "        )\n",
    "\n",
    "        context_builder_params = {\n",
    "            \"use_community_summary\": False,\n",
    "            \"shuffle_data\": True,\n",
    "            \"include_community_rank\": True,\n",
    "            \"min_community_rank\": 0,\n",
    "            \"community_rank_name\": \"rank\",\n",
    "            \"include_community_weight\": True,\n",
    "            \"community_weight_name\": \"occurrence weight\",\n",
    "            \"normalize_community_weight\": True,\n",
    "            \"max_tokens\": 20_000,\n",
    "            \"context_name\": \"Reports\",\n",
    "        }\n",
    "        map_llm_params = {\n",
    "            \"max_tokens\": 1000,\n",
    "            \"temperature\": 0.0,\n",
    "            \"response_format\": {\"type\": \"json_object\"},\n",
    "        }\n",
    "        reduce_llm_params = {\n",
    "            \"max_tokens\": 5000,\n",
    "            \"temperature\": 0.0,\n",
    "        }\n",
    "\n",
    "        self._search_engine = GlobalSearch(\n",
    "            model=chat_model,\n",
    "            callbacks=callbacks if callbacks is not None else [],\n",
    "            context_builder=_context_builder,\n",
    "            token_encoder=token_encoder,\n",
    "            max_data_tokens=12_000,\n",
    "            map_llm_params=map_llm_params,\n",
    "            reduce_llm_params=reduce_llm_params,\n",
    "            allow_general_knowledge=False,\n",
    "            json_mode=True,\n",
    "            context_builder_params=context_builder_params,\n",
    "            concurrent_coroutines=32,\n",
    "            response_type=\"multiple paragraphs\",\n",
    "        )\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        import asyncio\n",
    "        try:\n",
    "            loop = asyncio.get_running_loop()\n",
    "        except RuntimeError:\n",
    "            loop = None\n",
    "\n",
    "        if loop and loop.is_running():\n",
    "            # If in Jupyter, nest coroutine with ensure_future and run until complete\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            future = asyncio.ensure_future(self._search_engine.search(query))\n",
    "            result = loop.run_until_complete(future)\n",
    "        else:\n",
    "            result = asyncio.run(self._search_engine.search(query))\n",
    "        return result.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e93c6beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphrag_agent = Agent(\n",
    "    role=\"You are an expert in Microsoft Fabric product\",\n",
    "    goal=\"Your goal is to answer the user's questions about Microsoft Fabric.\",\n",
    "    backstory='As an expert in Microsoft Fabric, you have extensive knowledge about its features,' \n",
    "    ' functionalities, and best practices. You are here to assist users in understanding and utilizing Microsoft Fabric effectively.',\n",
    "    tools=[\n",
    "        LocalSearchTool(\n",
    "            input_dir=\"./output/before-tuning\",\n",
    "            llm_model=llm_model,\n",
    "            embedding_model=embedding_model,\n",
    "            api_key=api_key,\n",
    "            api_base=api_base,\n",
    "        ),\n",
    "            GlobalSearchTool(\n",
    "            input_dir=\"./output/before-tuning\",\n",
    "            llm_model=llm_model,\n",
    "            api_key=api_key,\n",
    "            api_base=api_base,\n",
    "        ),],\n",
    "    llm=LLM(model=f'azure/{llm_model}'),\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee2ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_instructions = \"\"\"\n",
    "You are an expert in Microsoft Fabric product. Your goal is to answer the user's question below about Microsoft Fabric.\n",
    "\n",
    "Users' question:\n",
    "{question}\n",
    "YOu must follow the below rules:\n",
    "1. You must use both tools provided to you to answer the user's questions.\n",
    "2- once you get the results from the tools, you must combine them and provide a final answer to the user.\n",
    "3- you must not repeat the texts or the contexts that are identical or very similar in both results.\n",
    "4- you must not provide the results of the tools to the user, you must only provide the final answer.\n",
    "5- if the user speficies that you need to use a specific tool, you must use that sepcific tool for exampl use local search you use the local search tool only.\n",
    "6- if the user asks what were the difference between the tool results, you must provide the differences between the two results. Otherwise, you must not provide the differences between the two results.\"\"\"\n",
    "\n",
    "\n",
    "async def execute_task(task_instructions):\n",
    "    task = Task(\n",
    "        description=task_instructions,\n",
    "        agent=graphrag_agent,\n",
    "        expected_output=\"A concise and accurate answer to the user's question about Microsoft Fabric.\"\n",
    "    \n",
    "    )\n",
    "    crew = Crew(agents=[graphrag_agent], tasks=[task])\n",
    "    result = crew.kickoff()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a229100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mYou are an expert in Microsoft Fabric product\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mWhat are the features of a pipeline in Fabric?\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: No community records added when building community context.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mYou are an expert in Microsoft Fabric product\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: To provide a concise and accurate answer about the features of a pipeline in Microsoft Fabric, I need to gather relevant information regarding pipelines. I will start by performing a local search to find specific details about pipeline features within Microsoft Fabric.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mlocal_search\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": \\\"features of a pipeline in Microsoft Fabric\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "### Overview of Pipelines in Microsoft Fabric\n",
      "\n",
      "Pipelines in Microsoft Fabric are a crucial component for automating data workflows and processes. They enable users to orchestrate the movement and transformation of data across various systems and environments, ensuring that data is efficiently processed and made available for analysis and decision-making [Data: Entities (517, 97)].\n",
      "\n",
      "### Key Features\n",
      "\n",
      "1. **Automation and Orchestration**: Pipelines facilitate the automation of data workflows, reducing the need for manual intervention. This automation is achieved through a series of structured activities and triggers that guide the data through its lifecycle, from collection to transformation and storage [Data: Entities (97)].\n",
      "\n",
      "2. **Integration with Other Tools**: Microsoft Fabric pipelines integrate seamlessly with other tools and services within the ecosystem, such as Data Factory and Deployment Pipelines. This integration allows for enhanced data processing capabilities and supports continuous integration and deployment (CI/CD) practices, which streamline the development and deployment of data solutions [Data: Relationships (2110, 2329)].\n",
      "\n",
      "3. **Support for Various Data Sources**: Pipelines in Microsoft Fabric can connect to a wide range of data sources, including Azure Data Lake Storage Gen2 and Azure HDInsight, enabling users to handle big data processing tasks effectively. This connectivity ensures that data can be ingested from multiple sources and processed in a unified manner [Data: Relationships (2332, 2333)].\n",
      "\n",
      "4. **Monitoring and Management**: The Fabric Monitor tool provides capabilities to monitor the performance and status of data pipelines, allowing users to filter and view pipelines with failures. This feature is essential for maintaining the reliability and efficiency of data workflows [Data: Entities (801)].\n",
      "\n",
      "### Advanced Capabilities\n",
      "\n",
      "Pipelines in Microsoft Fabric also support advanced features such as the execution of Spark job definitions and the use of SFTP connectors for secure data transfer. These capabilities enhance the flexibility and security of data processing workflows, making it easier for organizations to manage complex data operations [Data: Relationships (2334, 2335)].\n",
      "\n",
      "Overall, pipelines in Microsoft Fabric are designed to streamline data management processes, improve operational efficiency, and support the seamless integration of data across various platforms and environments. These features make them an indispensable tool for organizations looking to leverage their data assets effectively.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mYou are an expert in Microsoft Fabric product\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Pipelines in Microsoft Fabric are an essential component for automating data workflows and processes, enabling users to orchestrate data movement and transformation across various systems and environments. Key features include:\n",
      "\n",
      "1. **Automation and Orchestration**: Pipelines automate data workflows via structured activities and triggers, ensuring efficient data lifecycle management from collection to storage.\n",
      "\n",
      "2. **Integration with Other Tools**: They integrate seamlessly with Microsoft ecosystem tools, such as Data Factory and Deployment Pipelines, supporting enhanced data processing and continuous integration and deployment (CI/CD) practices.\n",
      "\n",
      "3. **Support for Various Data Sources**: Pipelines can connect to multiple data sources, including Azure Data Lake Storage Gen2 and Azure HDInsight, facilitating unified big data processing.\n",
      "\n",
      "4. **Monitoring and Management**: The Fabric Monitor tool allows users to monitor the performance and status of data pipelines, enhancing workflow reliability and efficiency.\n",
      "\n",
      "Advanced capabilities include executing Spark job definitions and using SFTP connectors for secure data transfers, providing flexibility and security in managing complex data operations. Overall, pipelines in Microsoft Fabric streamline data management processes and support seamless integration across platforms.\u001b[00m\n",
      "\n",
      "\n",
      "<class 'crewai.crews.crew_output.CrewOutput'>\n",
      "Pipelines in Microsoft Fabric are an essential component for automating data workflows and processes, enabling users to orchestrate data movement and transformation across various systems and environments. Key features include:\n",
      "\n",
      "1. **Automation and Orchestration**: Pipelines automate data workflows via structured activities and triggers, ensuring efficient data lifecycle management from collection to storage.\n",
      "\n",
      "2. **Integration with Other Tools**: They integrate seamlessly with Microsoft ecosystem tools, such as Data Factory and Deployment Pipelines, supporting enhanced data processing and continuous integration and deployment (CI/CD) practices.\n",
      "\n",
      "3. **Support for Various Data Sources**: Pipelines can connect to multiple data sources, including Azure Data Lake Storage Gen2 and Azure HDInsight, facilitating unified big data processing.\n",
      "\n",
      "4. **Monitoring and Management**: The Fabric Monitor tool allows users to monitor the performance and status of data pipelines, enhancing workflow reliability and efficiency.\n",
      "\n",
      "Advanced capabilities include executing Spark job definitions and using SFTP connectors for secure data transfers, providing flexibility and security in managing complex data operations. Overall, pipelines in Microsoft Fabric streamline data management processes and support seamless integration across platforms.\n"
     ]
    }
   ],
   "source": [
    "result = None\n",
    "async def main():\n",
    "    query_instruction = \"What are the features of a pipeline in Fabric?\"\n",
    "    result = await execute_task(query_instruction)\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "\n",
    "# Execute example\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ed0a10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dc46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
